import:
  - source: Alfresco/alfresco-build-tools:.travis.awscli_install.yml@v1.2.1
  - source: Alfresco/alfresco-build-tools:.travis.aws-iam-authenticator_install.yml@v1.2.1
  - source: Alfresco/alfresco-build-tools:.travis.kubernetes_install.yml@v1.2.1
  - source: Alfresco/alfresco-build-tools:.travis.helm_install.yml@v1.2.1

language: java
jdk: openjdk11

os: linux
dist: xenial

services:
  - docker

stages:
  - name: Lint
  - name: Build
  - name: Test
    if: commit_message !~ /\[skip tests\]/
  - name: Publish_Chart
    if: branch = master OR commit_message =~ /\[publish\]/
  - name: Release_Staging
    if: commit_message =~ /\[staging\]/
  - name: Release
    if: branch = master AND commit_message =~ /\[release\]/

env:
  global:
    - TRAVIS_WAIT_TIMEOUT=${TRAVIS_WAIT_TIMEOUT:-30}
    - BRANCH=${TRAVIS_PULL_REQUEST_BRANCH:-${TRAVIS_BRANCH}}
    - HELM_REPO_BASE_URL=https://kubernetes-charts.alfresco.com
    - HELM_REPO=${HELM_REPO:-incubator}

cache:
  directories:
    - ${HOME}/.m2
    - ${HOME}/.m2/repository

branches:
  only:
    - master
    - /AUTH-.*/
    - /OPSEXP-.*/

before_install:
  - |
    openssl aes-256-cbc -K $encrypted_e69bec42ae64_key -iv $encrypted_e69bec42ae64_iv -in test/scripts/config-files/realmRsaKeys.json.enc -out test/scripts/config-files/realmRsaKeys.json -d
    aws eks update-kubeconfig --name acs-cluster
    cp .travis.settings.xml $HOME/.m2/settings.xml

before_script:
  - |
    export PROJECT_NAME=alfresco-identity-service
    if [[ "${TRAVIS_BRANCH}" == "master" ]] && [[ "${TRAVIS_COMMIT_MESSAGE}" == *"[release]"* ]]
    then
      export HELM_REPO=stable
    fi
    export IDS_BUILD_NAME=$(echo ${TRAVIS_BRANCH} | cut -c1-28 | tr /_ - | tr -d [:punct:] | awk '{print tolower($0)}')-${TRAVIS_BUILD_NUMBER}

after_failure:
  - aws s3 rm s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/ --recursive

jobs:
  include:
    - name: Lint chart
      stage: Lint
      script: |
        helm dep up helm/${PROJECT_NAME}
        helm lint helm/${PROJECT_NAME}
    - name: Build
      stage: Build
      script: |
        source distribution/build.properties
        export IDENTITY_VERSION=${IDENTITY_VERSION}        
        echo "IDENTITY_VERSION=${IDENTITY_VERSION}"
        # build and package
        cd distribution
        make || { echo "Command failed with error code $?"; sleep 1; exit 1; }
        # upload ZIP file to S3 bucket
        aws s3 cp alfresco-identity-service-${IDENTITY_VERSION}.md5 s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/
        aws s3 cp alfresco-identity-service-${IDENTITY_VERSION}.zip s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/
    - name: Test ZIP on Linux
      stage: Test
      script: |
        source distribution/build.properties
        export IDENTITY_VERSION=${IDENTITY_VERSION}        
        echo "IDENTITY_VERSION=${IDENTITY_VERSION}"
        aws s3 cp s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/alfresco-identity-service-${IDENTITY_VERSION}.zip .
        ./distribution/tests/endpoints.sh
      #after_script: |
        # empty the artifacts bucket (in case there were previous failed builds)
        # aws s3 rm s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/ --recursive
    - name: Test Windows
      stage: Test
      os: windows
      language: shell
      env: 
        - OS=Windows
        - AWSCLI_PATH="C:\Program Files\Amazon\AWSCLIV2"
      before_install: |
        echo "skipping before_install"
      before_script: |
        echo "skipping before_script"
      install: |
        choco install awscli
      script: |
        source distribution/build.properties
        export IDENTITY_VERSION=${IDENTITY_VERSION}
        echo "IDENTITY_VERSION=${IDENTITY_VERSION}"
        export PATH=$AWSCLI_PATH:$PATH
        echo $PATH
        aws configure set aws_access_key_id ${AWS_SECRET_ACCESS_KEY}
        aws configure set aws_secret_access_key ${AWS_ACCESS_KEY_ID}
        aws configure set default.region ${AWS_DEFAULT_REGION}
        aws s3 ls s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/
        aws s3 ls s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/alfresco-identity-service-${IDENTITY_VERSION}.zip
        aws s3 cp s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/alfresco-identity-service-${IDENTITY_VERSION}.zip .
        unzip alfresco-identity-service-${IDENTITY_VERSION}.zip
        cd alfresco-identity-service-${IDENTITY_VERSION}/bin
        pwd
        dir
        export JAVA_HOME=${JAVA_HOME:-/c/jdk}
        export PATH=${JAVA_HOME}/bin:${PATH}
        choco install jdk8 -params 'installdir=c:\\jdk' -y        
        powershell -Command Get-ExecutionPolicy
        powershell -Command 'Set-ExecutionPolicy unrestricted'
        powershell -Command /c/Users/travis/build/Alfresco/alfresco-identity-service/distribution/tests/endpoints_ps.ps1
        powershell -Command /c/Users/travis/build/Alfresco/alfresco-identity-service/distribution/tests/endpoints_bat.ps1
    - name: Test Helm Chart
      stage: Test
      addons:
        chrome: stable
      script: |
        export namespace=${IDS_BUILD_NAME}
        export domain=dev.envalfresco.com
        export HOST=$namespace.$domain
        export release_name_ingress=ing-${TRAVIS_BUILD_NUMBER}
        export release_name_ids=ids-${TRAVIS_BUILD_NUMBER}
        export openldap_release=openldap-${TRAVIS_BUILD_NUMBER}

        # Utility Functions

        # pod status
        pod_status() {
          kubectl get pods --namespace $namespace -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,READY:.status.conditions[?\(@.type==\'Ready\'\)].status
        }

        # pods ready
        pods_ready() {
          PODS_COUNTER=0
          PODS_COUNTER_MAX=60
          PODS_SLEEP_SECONDS=10

          while [ "$PODS_COUNTER" -lt "$PODS_COUNTER_MAX" ]; do
            totalpods=$(pod_status | grep -v NAME | wc -l | sed 's/ *//')
            readypodcount=$(pod_status | grep ' True' | wc -l | sed 's/ *//')
            if [ "$readypodcount" -eq "$totalpods" ]; then
                  echo "     $readypodcount/$totalpods pods ready now"
                  pod_status
              echo "All pods are ready!"
              break
            fi
              PODS_COUNTER=$((PODS_COUNTER + 1))
              echo "just $readypodcount/$totalpods pods ready now - sleeping $PODS_SLEEP_SECONDS seconds - counter $PODS_COUNTER"
              sleep "$PODS_SLEEP_SECONDS"
              continue
            done

          if [ "$PODS_COUNTER" -ge "$PODS_COUNTER_MAX" ]; then
            pod_status
            echo "Pods did not start - exit 1"
            exit 1
          fi
        }

        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Namespace
        metadata:
          name: $namespace
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          name: $namespace:psp
          namespace: $namespace
        rules:
        - apiGroups:
          - policy
          resourceNames:
          - kube-system
          resources:
          - podsecuritypolicies
          verbs:
          - use
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: $namespace:psp:default
          namespace: $namespace
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: $namespace:psp
        subjects:
        - kind: ServiceAccount
          name: default
          namespace: $namespace
        ---
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: $namespace:psp:$release_name_ingress-nginx-ingress
          namespace: $namespace
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: $namespace:psp
        subjects:
        - kind: ServiceAccount
          name: $release_name_ingress-nginx-ingress
          namespace: $namespace
        ---
        $(kubectl create secret docker-registry quay-registry-secret --dry-run=client --docker-server=quay.io --docker-username=${DOCKER_USERNAME_QUAY} --docker-password=${DOCKER_PASSWORD_QUAY} -n $namespace -o yaml)
        EOF

        cat << EOR >> ingressvalues.yaml
        rbac:
          create: true
        controller:
          scope:
            enable: true
            namespace: $namespace
          admissionWebhooks:
            enabled: false
          config:
            proxy-body-size: "100m"
            generate-request-id: "true"
            PROXY-READ-TIMEOUT: "3600"
            proxy-send-timeout: "3600"
            ssl-redirect: "false"
            server-tokens: "false"
            use-forwarded-headers: "true"
          publishService:
            enabled: true
          service:
            targetPorts:
              https: http
              http: http
            annotations:
              external-dns.alpha.kubernetes.io/hostname: "$HOST"
              service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: "Creator=Platform-Services,Department=Engineering,NoAutomaticShutdown=True,Owner=Platform-Services,Production=False,Purpose=DBP and SSO Testing,Tool=k8sIngress"
              service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
              service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '3600'
              service.beta.kubernetes.io/aws-load-balancer-ssl-cert: $ACM_CERTIFICATE
              service.beta.kubernetes.io/aws-load-balancer-ssl-ports: https
              service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01
              service.beta.kubernetes.io/aws-load-balancer-security-groups: $AWS_SG
        EOR

        # install ingress-nginx
        helm upgrade --install $release_name_ingress --repo https://kubernetes.github.io/ingress-nginx ingress-nginx --version=3.7.1 -f ingressvalues.yaml \
          --wait \
          --namespace $namespace

        # install openldap
        helm upgrade --install $openldap_release --repo https://geek-cookbook.github.io/helm-openldap openldap --version 1.2.9 \
          -f test/scripts/ldap-config.yaml \
          --wait \
          --namespace $namespace

        # install Identity Service
        helm dep up helm/alfresco-identity-service
        helm upgrade --install $release_name_ids helm/alfresco-identity-service \
          --set ingress.hostName=$HOST \
          --set realm.alfresco.client.redirectUris[0]="https://${HOST}\*" \
          --set realm.alfresco.client.webOrigins[0]="https://${HOST}\*" \
          --wait \
          --namespace $namespace

        # check dns and pods
        DNS_PROPAGATED=0
        DNS_COUNTER=0
        DNS_COUNTER_MAX=90
        DNS_SLEEP_SECONDS=10

        echo "Trying to perform a trace DNS query to prevent caching"
        dig +trace $HOST @8.8.8.8

        while [ "$DNS_PROPAGATED" -eq 0 ] && [ "$DNS_COUNTER" -le "$DNS_COUNTER_MAX" ]; do
          host $HOST 8.8.8.8
          if [ "$?" -eq 1 ]; then
            DNS_COUNTER=$((DNS_COUNTER + 1))
            echo "DNS Not Propagated - Sleeping $DNS_SLEEP_SECONDS seconds"
            sleep "$DNS_SLEEP_SECONDS"
          else
            echo "DNS Propagated"
            DNS_PROPAGATED=1
          fi
        done

        [ $DNS_PROPAGATED -ne 1 ] && echo "DNS entry for $HOST did not propagate within expected time" && exit 1

        pods_ready

        # Set IDP Config 
        ./test/scripts/set_idp_config.sh
        postman_image=postman/newman_alpine33:3.9.2

        # run identity checks
        docker run -a STDOUT --volume $PWD/test/postman:/etc/newman --network host $postman_image run "identity-test-collection.json" --global-var "identity_host=$HOST"
        TEST_RESULT=$?
        echo "TEST_RESULT=${TEST_RESULT}"

        if [[ "${TEST_RESULT}" == "0" ]]; then
          docker run -a STDOUT --volume $PWD/test/postman:/etc/newman --network host $postman_image run "change-keycloak-access-token-lifespan-collection.json" --insecure --global-var "identity_host=$HOST"
          ./test/helm/delete_keycloak_pods.sh
          docker run -a STDOUT --volume $PWD/test/postman:/etc/newman --network host $postman_image run "check-keycloak-access-token-lifespan-change-persisted.json" --insecure --global-var "identity_host=$HOST"
          TEST_RESULT=$?
          echo "TEST_RESULT=${TEST_RESULT}"
        fi

        if [[ "${TEST_RESULT}" == "0" ]]; then
          docker run -a STDOUT --volume $PWD/test/postman:/etc/newman --network host $postman_image run "ldap-user-provider-tests.postman_collection.json" -d "ldap-test-data.json" --global-var "identity_host=$HOST"
          TEST_RESULT=$?
          echo "TEST_RESULT=${TEST_RESULT}"
        fi

        if [[ "${TEST_RESULT}" == "0" ]]; then
          cd test/scripts
          ./auth0-api.sh create $HOST https://$HOST
          ./configure-saml-ids.sh app_name=$HOST ids_base_url=https://$HOST
          cd ../saml 
          export KEYCLOAK_HOSTNAME=$HOST
          export KEYCLOAK_ISSUER=https://$HOST/auth/realms/alfresco
          mvn clean test
          TEST_RESULT=$?
          echo "TEST_RESULT=${TEST_RESULT}"
          cd ../..
        fi

        if [[ "${TRAVIS_COMMIT_MESSAGE}" != *"[keep env]"* ]]; then
          helm delete $release_name_ingress $release_name_ids $openldap_release -n $namespace
          kubectl delete namespace $namespace
          cd test/scripts
          ./auth0-api.sh delete $HOST
        fi

        if [[ "${TEST_RESULT}" == "1" ]]; then
          echo "Tests failed, exiting"
          exit 1
        fi
    - name: Test Upgrade
      stage: Test
      script: |
        cd test/saml
        mvn clean package -DskipTests
        ./upgrade/test-upgrade.sh

    - name: Package and Publish Chart
      stage: Publish_Chart
      script: |
        if [[ "${TRAVIS_BRANCH}" != "master" ]]; then
          CHART_VERSION=$(cat helm/${PROJECT_NAME}/Chart.yaml | grep version: | awk '{print $2}')
          # Only modify the chart version if it doesn't have '-M*' at the end
          if [[ "${CHART_VERSION}" != *"-M"* ]]; then
            NON_SUFFIX_VERSION=$(echo "${CHART_VERSION}" | awk -F - '{print $1}')
            ALPHA_BUILD_VERSION="${NON_SUFFIX_VERSION}-A${TRAVIS_BUILD_NUMBER}"
            echo "Changing Chart version to ${ALPHA_BUILD_VERSION} as this is a feature branch..."
            sed -i s,$CHART_VERSION,$ALPHA_BUILD_VERSION,g helm/${PROJECT_NAME}/Chart.yaml
          fi
        fi
        COMMIT_MESSAGE_FIRST_LINE=$(git log --pretty=format:%s --max-count=1)
        echo using COMMIT_MESSAGE_FIRST_LINE=${COMMIT_MESSAGE_FIRST_LINE}
        git clone https://${GITHUB_TOKEN}@github.com/Alfresco/charts.git
        echo using HELM_REPO=${HELM_REPO}
        mkdir repo
        helm package --dependency-update --destination repo helm/${PROJECT_NAME}
        helm repo index repo --url ${HELM_REPO_BASE_URL}/${HELM_REPO} --merge charts/${HELM_REPO}/index.yaml
        mv repo/* charts/${HELM_REPO}
        cd charts
        git add ${HELM_REPO}
        git commit -m "${COMMIT_MESSAGE_FIRST_LINE}"
        git push --quiet origin master
    - name: Publish to S3 Staging
      stage: Release_Staging
      before_deploy: |
        source distribution/build.properties
        export IDENTITY_VERSION=${IDENTITY_VERSION}
        echo "IDENTITY_VERSION=${IDENTITY_VERSION}"
        aws s3 cp s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/alfresco-identity-service-${IDENTITY_VERSION}.zip ./deploy_dir/alfresco-identity-service-${IDENTITY_VERSION}.zip
        aws s3 cp s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/alfresco-identity-service-${IDENTITY_VERSION}.md5 ./deploy_dir/alfresco-identity-service-${IDENTITY_VERSION}.md5
      language: bash
      deploy:
        provider: s3
        access_key_id: "${STAGING_AWS_ACCESS_KEY}"
        secret_access_key: "${STAGING_AWS_SECRET_KEY}"
        bucket: "${S3_STAGING_BUCKET}"
        skip_cleanup: true
        region: "eu-west-1"
        local_dir: "deploy_dir"
        upload-dir: "enterprise/alfresco-identity-service/${IDENTITY_VERSION}"
        on:
          all_branches: true
      after_script:
        - |
          if [[ "${TRAVIS_COMMIT_MESSAGE}" != *"[release]"* ]]
          then
            aws s3 rm s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/ --recursive
          fi
    - name: Publish to S3 and Nexus
      stage: Release
      before_deploy: |
        source distribution/build.properties
        export IDENTITY_VERSION=${IDENTITY_VERSION}
        echo "IDENTITY_VERSION=${IDENTITY_VERSION}"
        aws s3 cp s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/alfresco-identity-service-${IDENTITY_VERSION}.zip ./deploy_dir/alfresco-identity-service-${IDENTITY_VERSION}.zip
        aws s3 cp s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/alfresco-identity-service-${IDENTITY_VERSION}.md5 ./deploy_dir/alfresco-identity-service-${IDENTITY_VERSION}.md5
        aws s3 rm s3://${S3_ARTIFACTS_BUCKET}/build-${TRAVIS_BUILD_NUMBER}/ --recursive
        curl -q -u ${MAVEN_USERNAME}:${MAVEN_PASSWORD} --upload-file ./deploy_dir/alfresco-identity-service-${IDENTITY_VERSION}.zip https://artifacts.alfresco.com/nexus/content/repositories/enterprise-releases/org/alfresco/alfresco-identity-service/${IDENTITY_VERSION}/alfresco-identity-service-${IDENTITY_VERSION}.zip
      language: bash
      deploy:
        provider: s3
        access_key_id: "${RELEASE_AWS_ACCESS_KEY}"
        secret_access_key: "${RELEASE_AWS_SECRET_KEY}"
        bucket: "${S3_RELEASE_BUCKET}"
        skip_cleanup: true
        region: "eu-west-1"
        local_dir: "deploy_dir"
        upload-dir: "release/enterprise/alfresco-identity-service/${IDENTITY_VERSION}"
        on:
          all_branches: true
    - name: Publish DockerHub
      stage: Release
      script: |
        source distribution/build.properties
        export IDENTITY_VERSION=${IDENTITY_VERSION}
        echo "IDENTITY_VERSION=${IDENTITY_VERSION}"
        export PRIVATE_IMAGE=quay.io/alfresco/alfresco-identity-service:${IDENTITY_VERSION}
        export PUBLIC_IMAGE=alfresco/alfresco-identity-service:${IDENTITY_VERSION}
        echo "$DOCKER_PASSWORD_QUAY" | docker login quay.io -u "$DOCKER_USERNAME_QUAY" --password-stdin
        docker pull ${PRIVATE_IMAGE}
        docker tag ${PRIVATE_IMAGE} ${PUBLIC_IMAGE}
        echo "$DOCKER_PASSWORD" | docker login -u "$DOCKER_USERNAME" --password-stdin
        docker push ${PUBLIC_IMAGE}
